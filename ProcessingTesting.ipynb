{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tika import parser\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import en_core_web_sm  # or any other model you downloaded via spacy download or pip\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up PDF file - requires the DejaVu font to be installed in a fonts folder in the\n",
    "# fpdf package directory in the python environment\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.add_font('DejaVu', '', 'DejaVuSans.ttf', uni=True)\n",
    "pdf.add_font('DejaVuSans-Bold', '', 'DejaVuSans-Bold.ttf', uni=True)\n",
    "pdf.add_font('DejaVuSans-Oblique', '', 'DejaVuSans-Oblique.ttf', uni=True)\n",
    "pdf.add_font('DejaVuSans-BoldOblique', '', 'DejaVuSans-BoldOblique.ttf', uni=True)\n",
    "pdf.set_font('DejaVuSans-Bold', '', 14)\n",
    "pdf.cell(w=0, txt=\"Output Report\", ln=1, align=\"C\")\n",
    "pdf.ln(20)\n",
    "\n",
    "# Load the model to be used by SpaCy for Named Entity Recognition\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Set up a stemmer to use to stem words\n",
    "pstemmer = PorterStemmer()\n",
    "\n",
    "# Set up input path, stop words to be excluded and keywords to be matched\n",
    "input_path = 'C:\\\\t2'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "keywords = ['IS', 'terrorism', 'bomb', 'is', 'the', 'consortium']\n",
    "\n",
    "# Filter out stopwords from keywords list, POS tag keywords\n",
    "filterkeywords = [w for w in keywords if w not in stop_words]\n",
    "poskeywords = nltk.pos_tag(filterkeywords)\n",
    "\n",
    "# If the first keyword is a verb, move it and reparse the list\n",
    "# This prevents verbs that may also be nouns being misidentified\n",
    "if poskeywords[0][1] == 'VBZ':\n",
    "    filterkeywords.insert(1, filterkeywords.pop(0))\n",
    "    poskeywords = nltk.pos_tag(filterkeywords)\n",
    "\n",
    "# Build a list of stem keywords for matching\n",
    "stemkeywords = nltk.pos_tag([pstemmer.stem(t) for t in filterkeywords])\n",
    "\n",
    "# Set up Dataframe - this will hold all the documents and the scores\n",
    "d = pd.DataFrame()\n",
    "\n",
    "# Create a list to use for clustering - this is for topic modelling\n",
    "doclist = []\n",
    "word_matches = defaultdict(list)\n",
    "\n",
    "\n",
    "# Use Tika to parse the file\n",
    "def parsewithtika(inputfile):\n",
    "    parsed = parser.from_file(inputfile)\n",
    "    # Extract the text content from the parsed file\n",
    "    psd = parsed[\"content\"]\n",
    "    return re.sub(r'\\s+', ' ', psd)\n",
    "\n",
    "\n",
    "# Language filter - removes non english documents from the list\n",
    "def filterlanguage(inputfile):\n",
    "    if detect(inputfile) != 'en':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Get parts of speech from SpaCy\n",
    "def pos(x):\n",
    "    return [(token.text, token.tag_) for token in x]\n",
    "\n",
    "\n",
    "# Add the parts of speech to the words\n",
    "def spacy_pos(x):\n",
    "    pos_sent = []\n",
    "    for sentence in x:\n",
    "        processed_spacy = nlp(sentence)\n",
    "        pos_sent.append(pos(processed_spacy))\n",
    "    return pos_sent\n",
    "\n",
    "\n",
    "# Add NER tags to words, and return the set so we don't have duplicates\n",
    "def ner(x):\n",
    "    ents = []\n",
    "    for sentence in x:\n",
    "        processed_spacy = nlp(sentence)\n",
    "        for ent in processed_spacy.ents:\n",
    "            ents.append((ent.text, ent.label_))\n",
    "    return set(ents)\n",
    "\n",
    "\n",
    "# Word tokens, parts of speech tagging\n",
    "def wordtokens(dataframe):\n",
    "    # Get all the words\n",
    "    dataframe['words'] = (dataframe['sentences'].apply(lambda x: [word_tokenize(item) for item in x]))\n",
    "    # Get all the parts of speech tags\n",
    "    dataframe['pos'] = dataframe['sentences'].map(spacy_pos)\n",
    "    # Get all the named entity tags\n",
    "    dataframe['ner'] = dataframe['sentences'].map(ner)\n",
    "    # Lowercase every word and put them all in a single list for each document\n",
    "    dataframe['allwords'] = dataframe['words'].apply(lambda x: [item.strip(string.punctuation).lower()\n",
    "                                                                for sublist in x for item in sublist])\n",
    "    # Strip out non words and stop words\n",
    "    dataframe['allwords'] = (dataframe['allwords'].apply(lambda x: [item for item in x if item.isalpha()\n",
    "                                                                    and item not in stop_words]))\n",
    "    # Calculate the frequency of each word in the document\n",
    "    dataframe['mfreq'] = dataframe['allwords'].apply(nltk.FreqDist)\n",
    "    # Get all the pos tagged words in a single list for each document\n",
    "    dataframe['poslist'] = dataframe['pos'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "    # Calculate the frequency of each pos tagged word\n",
    "    dataframe['mfreqpos'] = dataframe['poslist'].apply(nltk.FreqDist)\n",
    "    # Get the stems of all the words\n",
    "    dataframe['stemwords'] = dataframe['allwords'].apply(lambda x: [pstemmer.stem(item) for item in x])\n",
    "    # Calculate frequency of stemmed words\n",
    "    dataframe['mfreqstem'] = dataframe['stemwords'].apply(nltk.FreqDist)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Score documents based on cleansed dataset - so should discount stopwords and be sensible\n",
    "def scoring(dataframe, list):\n",
    "    for word in keywords:\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            if word in row['allwords']:\n",
    "                if not row['document'] in list[word]:\n",
    "                    list[word].append(row['document'])\n",
    "                    dataframe.loc[idx, 'score'] += (row['mfreq'][word] * 0.75)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Score documents based on pos - should be most exact match\n",
    "def scoringpos(dataframe, list):\n",
    "    for (w1, t1) in poskeywords:\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            if (w1, t1) in row['poslist']:\n",
    "                if not row['document'] in list[w1]:\n",
    "                    list[w1].append(row['document'])\n",
    "                    dataframe.loc[idx, 'score'] += row['mfreqpos'][(w1, t1)]\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Score documents based on stemmed words in cleansed dataset - so should discount stopwords and be sensible\n",
    "def scoringstem(dataframe, list):\n",
    "    for word in stemkeywords:\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            if word in row['stemwords']:\n",
    "                if not row['document'] in list[word]:\n",
    "                    list[word].append(row['document'])\n",
    "                    dataframe.loc[idx, 'score'] += (row['mfreqstem'][word] * 0.5)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Find keywords using POS, show the sentence the word was found in\n",
    "def contextkeywords(dataframe):\n",
    "    pdf.set_font('DejaVuSans-Bold', '', 12)\n",
    "    pdf.cell(w=0,txt=\"Here are the exact keyword matches in context: \", ln=1, align=\"L\")\n",
    "    pdf.ln(10)\n",
    "    for (w1, t1) in poskeywords:\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            for index, r in enumerate(row['pos']):\n",
    "                if (w1, t1) in r:\n",
    "                    pdf.set_font('DejaVuSans-Bold', '', 10)\n",
    "                    pdf.multi_cell(w=0, h=10, txt=row['document'] + ' - ', align=\"L\")\n",
    "                    pdf.set_font('DejaVu', '', 10)\n",
    "                    pdf.multi_cell(w=0, h=10, txt=' '.join(row['words'][index]),  align=\"L\")\n",
    "                    pdf.ln(5)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Show all the documents that had keyword matches, for each keyword\n",
    "def printkeywordmatches(list):\n",
    "    pdf.set_font('DejaVuSans-Bold', '', 12)\n",
    "    pdf.cell(w=0, txt=\"Keyword match results: \", ln=1, align=\"L\")\n",
    "    pdf.ln(10)\n",
    "    for key, val in list.items():\n",
    "        pdf.set_font('DejaVuSans-Bold', '', 10)\n",
    "        pdf.multi_cell(w=0, h=10, txt=\"Documents containing keyword: \" + key, align=\"L\")\n",
    "        pdf.ln(5)\n",
    "        pdf.set_font('DejaVu', '', 10)\n",
    "        pdf.multi_cell(w=0, h=10, txt=', '.join(val), align=\"L\")\n",
    "        pdf.ln(10)\n",
    "\n",
    "\n",
    "# tokenize each word in the text and then filter out non alphabet words, then get all the stems\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [pstemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# Cluster documents and demonstrate prediction\n",
    "# K means clustering using the following parameters:\n",
    "# - filter out english stopwords\n",
    "# - word must appear in no more than 80% of documents\n",
    "# - word must appear in no less than 20% of documents\n",
    "# - token and stem words, allow up to 3 words together as a grouping\n",
    "# TODO - calculate ideal k value\n",
    "def clustering(documents):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=0.2, use_idf=True,\n",
    "                                 tokenizer=tokenize_and_stem, ngram_range=(1, 3))\n",
    "    X = vectorizer.fit_transform(doclist)\n",
    "\n",
    "    true_k = 5\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    model.fit(X)\n",
    "\n",
    "    print(\"Top terms per cluster:\")\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i),\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind]),\n",
    "        print\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Prediction\")\n",
    "\n",
    "    Y = vectorizer.transform([\"this is a document about islamic state \"\n",
    "                              \"and terrorists and bombs IS jihad terrorism isil\"])\n",
    "    prediction = model.predict(Y)\n",
    "    print(\"A document with 'bad' terms would be in:\")\n",
    "    print(prediction)\n",
    "\n",
    "    Y = vectorizer.transform([\"completely innocent text just about kittens and puppies\"])\n",
    "    prediction = model.predict(Y)\n",
    "    print(\"A document with 'good' terms would be in:\")\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing - the following files have been processed:\n",
      "01,-,Good,bank,statement.pdf\n",
      "031918comments2.authcheckdam.pdf\n",
      "881961_CHECKLIST-2014_rev62714.pdf\n",
      "bank-reconciliation-example.pdf\n",
      "Bishop_Book_4_eBook.pdf\n",
      "britain_mag_media_pack.pdf\n",
      "c07Chemicalreactions_WEB.pdf\n",
      "cassandra_thedefinitiveguide.pdf\n",
      "children result( Individula and together ) v1 7-3-16.docx\n",
      "Correct bank statement.pdf\n",
      "D3S_EN.pdf\n",
      "datascienceatthecommandline.pdf\n",
      "dis5790_parrainage_mmf_a5_4.pdf\n",
      "DomesticWireFunds.pdf\n",
      "DTM_AprMay_2018.pdf\n",
      "dubai 1 2.pdf\n",
      "Early social interaction project for childen with autism   begining in the second year of life (1) 2.pdf\n",
      "eng[1].htm\n",
      "eula.1036.txt\n",
      "Factors-Affecting-Rate-of-Reaction.pdf\n",
      "Fireworks!-ConcertInPark08.pdf\n",
      "HERO5Black_UM_ENG_REVC_Web.pdf\n",
      "iphone  en.pdf\n",
      "Kaplan, Andreas - Users of the world, unite.pdf\n",
      "Kuwait job.docx\n",
      "learningspark.pdf\n",
      "log.txt\n",
      "manual_charge_2_en_US.pdf\n",
      "Memes-and-the-evolution-of-religion-We-need-memetics-too.pdf\n",
      "Mohamed Salem  Religion, Spirituality and Psychiatry.pdf\n",
      "MSAB_License_Management_Brazilian Portuguese.pdf\n",
      "MSAB_License_Management_Chinese.pdf\n",
      "MSAB_License_Management_English.pdf\n",
      "MSAB_License_Management_French.pdf\n",
      "MSAB_License_Management_German.pdf\n",
      "MSAB_License_Management_Japanese.pdf\n",
      "MSAB_License_Management_Russian.pdf\n",
      "MSAB_License_Management_Spanish.pdf\n",
      "MSAB_License_Management_Turkish.pdf\n",
      "nutritionals.pdf\n",
      "oa_bitc_jargon_buster.pdf\n",
      "Order Confirmation.pdf\n",
      "Orderconf.pdf\n",
      "P857_ImportantInformation-TermsAndConditions.pdf\n",
      "Patient+Type+2+opt-out+letter+v1.0.pdf\n",
      "Periodic-Table-Chemical-Reactions-Summary1.pdf\n",
      "Philosophy of Religion.pdf\n",
      "pwd-coursework-II-scalability testing.pdf\n",
      "r003.pdf\n",
      "README.txt\n",
      "ReferenceCard.pdf\n",
      "ReferenceCardForMac.pdf\n",
      "Religion-Security-Global-Uncertainties.pdf\n",
      "Results and comments.docx\n",
      "Sample Bank Statement.pdf\n",
      "Sample Grade and Receipt Documents.pdf\n",
      "sample-bank-statement.pdf\n",
      "south_cambs_magazine_summer_2018small.pdf\n",
      "sql-code-smells.pdf\n",
      "STARTBackgroundReport_TerrorisminOlympicsSochiRussia_Jan2014.pdf\n",
      "STARTCongressionalTestimony_StateofAQandAffiliates_WilliamBraniff.pdf\n",
      "STARTResearchBrief_Anatomizing.pdf\n",
      "STARTResearchBrief_CommunityPolicing_Feb2015.pdf\n",
      "STARTSymposium2015_CounterterrorismPanel.pdf\n",
      "STARTSymposium2015_IndividualRadicalizationPanel.pdf\n",
      "START_CounteringInhumane_ResearchBrief_May2015.pdf\n",
      "START_CSTAB_ECDB_25YearsofIdeologicalHomicideVictimizationUS_March2016.pdf\n",
      "START_CSTAB_JihadiIndustryAssessingOrganizationalLeadershipCyberProfiles_July2017.pdf\n",
      "START_CSTAB_ReactionsWaronTerrorism_Feb2017.pdf\n",
      "START_CSTAB_USMuslimOpinionsAboutISISSyriaUSElection_June2017.pdf\n",
      "START_DemystifyingGrayZoneConflict_Libya_Nov2016.pdf\n",
      "START_DHS_SyriaBarometerSurvey_30June2016.pdf\n",
      "START_ECDB_FinancialCrimesSchemesPerpetratedbyFarRightExtremists_June2015.pdf\n",
      "START_Gruenewald_FarRightHomicideLoners.pdf\n",
      "START_GTD_OverviewofTerrorism2014_Aug2015.pdf\n",
      "START_ISIL_Lesson2_AnOrganizationalProfileoftheIslamicState.pdf\n",
      "START_ISIL_Part3_Script.pdf\n",
      "START_IUSSD_GTDTerroristAttacksinUS_ResearchHighlight_Jan2014.pdf\n",
      "START_JihadistTerroristPlotsUS_Dec2017.pdf\n",
      "START_LessonsLearnedfromMentalHealthAndEducation_EducatorSummary_Oct2015.pdf\n",
      "START_McCauley_PsychologyofLoneActorTerrorists.pdf\n",
      "START_PIRUS_ResearchBrief_Sept2017.pdf\n",
      "START_ResearchBrief_HateCrimeTerror_May2016.pdf\n",
      "START_Smith_GeospatialTemporalPatternsofLoneActorTerrorism.pdf\n",
      "START_TerrorismEnergyAttacks_ResearchBrief_June2015.pdf\n",
      "START_UnderstandingLawEnforcementIntelligenceProcesses_July2014.pdf\n",
      "START_UnderstandingLoneActorTerrorism_ResearchHighlight_Oct2013.pdf\n",
      "START_Webber_EvaluatingJihadistNarratives.pdf\n",
      "tech & cultural appropriation 2012.pdf\n",
      "ten-point-travel.pdf\n",
      "Ticket_Prices_FulhamBroadway.pdf\n",
      "tips_users_chemicals_workplace_en.pdf\n",
      "travel-conditions-gotogate-uk-20150609.pdf\n",
      "UAE job.htm\n",
      "Using the Python Shell.pdf\n",
      "water-companies-letter-SoS-to-Ofwat-180131.pdf\n",
      "webcolors.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                             document  score\n",
      "58    START_CSTAB_ReactionsWaronTerrorism_Feb2017.pdf  87.25\n",
      "53       STARTSymposium2015_CounterterrorismPanel.pdf  66.75\n",
      "77      START_Webber_EvaluatingJihadistNarratives.pdf  47.25\n",
      "61      START_DHS_SyriaBarometerSurvey_30June2016.pdf  42.75\n",
      "59  START_CSTAB_USMuslimOpinionsAboutISISSyriaUSEl...  41.25\n",
      "60  START_DemystifyingGrayZoneConflict_Libya_Nov20...  38.25\n",
      "75  START_UnderstandingLawEnforcementIntelligenceP...  34.00\n",
      "54  STARTSymposium2015_IndividualRadicalizationPan...  26.00\n",
      "73  START_Smith_GeospatialTemporalPatternsofLoneAc...  18.00\n",
      "76  START_UnderstandingLoneActorTerrorism_Research...  16.50\n",
      "49  STARTBackgroundReport_TerrorisminOlympicsSochi...  14.75\n",
      "56  START_CSTAB_ECDB_25YearsofIdeologicalHomicideV...  12.25\n",
      "57  START_CSTAB_JihadiIndustryAssessingOrganizatio...  11.00\n",
      "50  STARTCongressionalTestimony_StateofAQandAffili...  10.50\n",
      "70  START_McCauley_PsychologyofLoneActorTerrorists...  10.25\n",
      "63        START_Gruenewald_FarRightHomicideLoners.pdf  10.00\n",
      "72    START_ResearchBrief_HateCrimeTerror_May2016.pdf   8.75\n",
      "64      START_GTD_OverviewofTerrorism2014_Aug2015.pdf   7.75\n",
      "42         Religion-Security-Global-Uncertainties.pdf   7.00\n",
      "65  START_ISIL_Lesson2_AnOrganizationalProfileofth...   6.25\n",
      "51                 STARTResearchBrief_Anatomizing.pdf   5.00\n",
      "67  START_IUSSD_GTDTerroristAttacksinUS_ResearchHi...   5.00\n",
      "68         START_JihadistTerroristPlotsUS_Dec2017.pdf   4.75\n",
      "74  START_TerrorismEnergyAttacks_ResearchBrief_Jun...   2.75\n",
      "62  START_ECDB_FinancialCrimesSchemesPerpetratedby...   1.75\n",
      "66                        START_ISIL_Part3_Script.pdf   1.75\n",
      "69  START_LessonsLearnedfromMentalHealthAndEducati...   1.75\n",
      "71             START_PIRUS_ResearchBrief_Sept2017.pdf   1.75\n",
      "55  START_CounteringInhumane_ResearchBrief_May2015...   1.75\n",
      "52   STARTResearchBrief_CommunityPolicing_Feb2015.pdf   1.75\n",
      "..                                                ...    ...\n",
      "11                    datascienceatthecommandline.pdf   0.00\n",
      "10                                         D3S_EN.pdf   0.00\n",
      "9                          Correct bank statement.pdf   0.00\n",
      "8   children result( Individula and together ) v1 ...   0.00\n",
      "7                    cassandra_thedefinitiveguide.pdf   0.00\n",
      "5                          britain_mag_media_pack.pdf   0.00\n",
      "4                             Bishop_Book_4_eBook.pdf   0.00\n",
      "3                     bank-reconciliation-example.pdf   0.00\n",
      "2                  881961_CHECKLIST-2014_rev62714.pdf   0.00\n",
      "21                                     iphone  en.pdf   0.00\n",
      "22    Kaplan, Andreas - Users of the world, unite.pdf   0.00\n",
      "23                                    Kuwait job.docx   0.00\n",
      "34             Patient+Type+2+opt-out+letter+v1.0.pdf   0.00\n",
      "1                    031918comments2.authcheckdam.pdf   0.00\n",
      "41                            ReferenceCardForMac.pdf   0.00\n",
      "40                                  ReferenceCard.pdf   0.00\n",
      "39                                         README.txt   0.00\n",
      "38                                           r003.pdf   0.00\n",
      "37          pwd-coursework-II-scalability testing.pdf   0.00\n",
      "35     Periodic-Table-Chemical-Reactions-Summary1.pdf   0.00\n",
      "33   P857_ImportantInformation-TermsAndConditions.pdf   0.00\n",
      "24                                  learningspark.pdf   0.00\n",
      "32                             Order Confirmation.pdf   0.00\n",
      "30                                   nutritionals.pdf   0.00\n",
      "29                MSAB_License_Management_English.pdf   0.00\n",
      "28  Mohamed Salem  Religion, Spirituality and Psyc...   0.00\n",
      "27  Memes-and-the-evolution-of-religion-We-need-me...   0.00\n",
      "26                          manual_charge_2_en_US.pdf   0.00\n",
      "25                                            log.txt   0.00\n",
      "86                                      webcolors.txt   0.00\n",
      "\n",
      "[87 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Main loop function\n",
    "# Iterate over all files in the folder and process each one in turn\n",
    "print('Starting processing - the following files have been processed:')\n",
    "for input_file in glob.glob(os.path.join(input_path, '*.*')):\n",
    "    # Grab the file name\n",
    "    filename = os.path.basename(input_file)\n",
    "    fname = os.path.splitext(filename)[0]\n",
    "    print(filename)\n",
    "\n",
    "    # Parse the file to get to the text\n",
    "    parsed = parsewithtika(input_file)\n",
    "\n",
    "    # Language detection algorithm is non - deterministic, which means that if you try to run it on a text which is\n",
    "    # either too short or too ambiguous, you might get different results every time you run it\n",
    "    if filterlanguage(parsed):\n",
    "        continue\n",
    "\n",
    "    # Ignore any documents with <100 words\n",
    "    if len(parsed) < 100:\n",
    "        continue\n",
    "\n",
    "    # Create doclist for use in topic modelling\n",
    "    doclist.append(parsed)\n",
    "    # Sentence fragments\n",
    "    sentences = sent_tokenize(parsed)\n",
    "\n",
    "    # Build up dataframe\n",
    "    temp = pd.Series([filename, sentences])\n",
    "    d = d.append(temp, ignore_index=True)\n",
    "\n",
    "print('\\n')\n",
    "d.reset_index(drop=True, inplace=True)\n",
    "d.columns = ['document', 'sentences']\n",
    "\n",
    "\n",
    "# Word tokenize the sentences, cleanup, parts of speech tagging\n",
    "wordtokens(d)\n",
    "d['score'] = 0\n",
    "\n",
    "# Now we score in a calculated manner:\n",
    "# Score 1 for matching word (case sensitive and POS)\n",
    "scoringpos(d, word_matches)\n",
    "# Score 0.75 for matching word (case insensitive,  stop words removed)\n",
    "scoring(d, word_matches)\n",
    "# Score 0.5 for matching stem of word (case insensitive, stop words removed)\n",
    "scoringstem(d, word_matches)\n",
    "# Print out the results of keyword matching\n",
    "printkeywordmatches(word_matches)\n",
    "# Find words in context with POS\n",
    "contextkeywords(d)\n",
    "\n",
    "# Sort by scoring\n",
    "d = d.sort_values('score', ascending=False)\n",
    "\n",
    "# Print sorted documents\n",
    "print('\\n')\n",
    "pdf.ln(10)\n",
    "pdf.set_font('DejaVuSans-Bold', '', 12)\n",
    "pdf.cell(w=0,txt=\"Here are the scores based on cleansed data: \", ln=1, align=\"L\")\n",
    "pdf.ln(5)\n",
    "pdf.set_font('DejaVu', '', 10)\n",
    "print(d[['document', 'score']])\n",
    "\n",
    "# Effective page width, or just epw\n",
    "epw = pdf.w - 2 * pdf.l_margin\n",
    "\n",
    "# Set column width to 1/4 of effective page width to distribute content\n",
    "# evenly across table and page\n",
    "col_width = epw / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text height is the same as current font size\n",
    "th = pdf.font_size\n",
    "data = d[['document', 'score']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['START_CSTAB_ReactionsWaronTerrorism_Feb2017.pdf', 87.25],\n",
       "       ['STARTSymposium2015_CounterterrorismPanel.pdf', 66.75],\n",
       "       ['START_Webber_EvaluatingJihadistNarratives.pdf', 47.25],\n",
       "       ['START_DHS_SyriaBarometerSurvey_30June2016.pdf', 42.75],\n",
       "       ['START_CSTAB_USMuslimOpinionsAboutISISSyriaUSElection_June2017.pdf',\n",
       "        41.25],\n",
       "       ['START_DemystifyingGrayZoneConflict_Libya_Nov2016.pdf', 38.25],\n",
       "       ['START_UnderstandingLawEnforcementIntelligenceProcesses_July2014.pdf',\n",
       "        34.0],\n",
       "       ['STARTSymposium2015_IndividualRadicalizationPanel.pdf', 26.0],\n",
       "       ['START_Smith_GeospatialTemporalPatternsofLoneActorTerrorism.pdf',\n",
       "        18.0],\n",
       "       ['START_UnderstandingLoneActorTerrorism_ResearchHighlight_Oct2013.pdf',\n",
       "        16.5],\n",
       "       ['STARTBackgroundReport_TerrorisminOlympicsSochiRussia_Jan2014.pdf',\n",
       "        14.75],\n",
       "       ['START_CSTAB_ECDB_25YearsofIdeologicalHomicideVictimizationUS_March2016.pdf',\n",
       "        12.25],\n",
       "       ['START_CSTAB_JihadiIndustryAssessingOrganizationalLeadershipCyberProfiles_July2017.pdf',\n",
       "        11.0],\n",
       "       ['STARTCongressionalTestimony_StateofAQandAffiliates_WilliamBraniff.pdf',\n",
       "        10.5],\n",
       "       ['START_McCauley_PsychologyofLoneActorTerrorists.pdf', 10.25],\n",
       "       ['START_Gruenewald_FarRightHomicideLoners.pdf', 10.0],\n",
       "       ['START_ResearchBrief_HateCrimeTerror_May2016.pdf', 8.75],\n",
       "       ['START_GTD_OverviewofTerrorism2014_Aug2015.pdf', 7.75],\n",
       "       ['Religion-Security-Global-Uncertainties.pdf', 7.0],\n",
       "       ['START_ISIL_Lesson2_AnOrganizationalProfileoftheIslamicState.pdf',\n",
       "        6.25],\n",
       "       ['STARTResearchBrief_Anatomizing.pdf', 5.0],\n",
       "       ['START_IUSSD_GTDTerroristAttacksinUS_ResearchHighlight_Jan2014.pdf',\n",
       "        5.0],\n",
       "       ['START_JihadistTerroristPlotsUS_Dec2017.pdf', 4.75],\n",
       "       ['START_TerrorismEnergyAttacks_ResearchBrief_June2015.pdf', 2.75],\n",
       "       ['START_ECDB_FinancialCrimesSchemesPerpetratedbyFarRightExtremists_June2015.pdf',\n",
       "        1.75],\n",
       "       ['START_ISIL_Part3_Script.pdf', 1.75],\n",
       "       ['START_LessonsLearnedfromMentalHealthAndEducation_EducatorSummary_Oct2015.pdf',\n",
       "        1.75],\n",
       "       ['START_PIRUS_ResearchBrief_Sept2017.pdf', 1.75],\n",
       "       ['START_CounteringInhumane_ResearchBrief_May2015.pdf', 1.75],\n",
       "       ['STARTResearchBrief_CommunityPolicing_Feb2015.pdf', 1.75],\n",
       "       ['c07Chemicalreactions_WEB.pdf', 1.0],\n",
       "       ['eula.1036.txt', 1.0],\n",
       "       ['Philosophy of Religion.pdf', 0.75],\n",
       "       ['oa_bitc_jargon_buster.pdf', 0.75],\n",
       "       ['sql-code-smells.pdf', 0.0],\n",
       "       ['01,-,Good,bank,statement.pdf', 0.0],\n",
       "       ['sample-bank-statement.pdf', 0.0],\n",
       "       ['tech & cultural appropriation 2012.pdf', 0.0],\n",
       "       ['ten-point-travel.pdf', 0.0],\n",
       "       ['Ticket_Prices_FulhamBroadway.pdf', 0.0],\n",
       "       ['tips_users_chemicals_workplace_en.pdf', 0.0],\n",
       "       ['travel-conditions-gotogate-uk-20150609.pdf', 0.0],\n",
       "       ['UAE job.htm', 0.0],\n",
       "       ['Using the Python Shell.pdf', 0.0],\n",
       "       ['water-companies-letter-SoS-to-Ofwat-180131.pdf', 0.0],\n",
       "       ['south_cambs_magazine_summer_2018small.pdf', 0.0],\n",
       "       ['Results and comments.docx', 0.0],\n",
       "       ['Sample Grade and Receipt Documents.pdf', 0.0],\n",
       "       ['Sample Bank Statement.pdf', 0.0],\n",
       "       ['HERO5Black_UM_ENG_REVC_Web.pdf', 0.0],\n",
       "       ['Fireworks!-ConcertInPark08.pdf', 0.0],\n",
       "       ['Factors-Affecting-Rate-of-Reaction.pdf', 0.0],\n",
       "       ['eng[1].htm', 0.0],\n",
       "       ['Early social interaction project for childen with autism   begining in the second year of life (1) 2.pdf',\n",
       "        0.0],\n",
       "       ['dubai 1 2.pdf', 0.0],\n",
       "       ['DTM_AprMay_2018.pdf', 0.0],\n",
       "       ['DomesticWireFunds.pdf', 0.0],\n",
       "       ['datascienceatthecommandline.pdf', 0.0],\n",
       "       ['D3S_EN.pdf', 0.0],\n",
       "       ['Correct bank statement.pdf', 0.0],\n",
       "       ['children result( Individula and together ) v1 7-3-16.docx', 0.0],\n",
       "       ['cassandra_thedefinitiveguide.pdf', 0.0],\n",
       "       ['britain_mag_media_pack.pdf', 0.0],\n",
       "       ['Bishop_Book_4_eBook.pdf', 0.0],\n",
       "       ['bank-reconciliation-example.pdf', 0.0],\n",
       "       ['881961_CHECKLIST-2014_rev62714.pdf', 0.0],\n",
       "       ['iphone  en.pdf', 0.0],\n",
       "       ['Kaplan, Andreas - Users of the world, unite.pdf', 0.0],\n",
       "       ['Kuwait job.docx', 0.0],\n",
       "       ['Patient+Type+2+opt-out+letter+v1.0.pdf', 0.0],\n",
       "       ['031918comments2.authcheckdam.pdf', 0.0],\n",
       "       ['ReferenceCardForMac.pdf', 0.0],\n",
       "       ['ReferenceCard.pdf', 0.0],\n",
       "       ['README.txt', 0.0],\n",
       "       ['r003.pdf', 0.0],\n",
       "       ['pwd-coursework-II-scalability testing.pdf', 0.0],\n",
       "       ['Periodic-Table-Chemical-Reactions-Summary1.pdf', 0.0],\n",
       "       ['P857_ImportantInformation-TermsAndConditions.pdf', 0.0],\n",
       "       ['learningspark.pdf', 0.0],\n",
       "       ['Order Confirmation.pdf', 0.0],\n",
       "       ['nutritionals.pdf', 0.0],\n",
       "       ['MSAB_License_Management_English.pdf', 0.0],\n",
       "       ['Mohamed Salem  Religion, Spirituality and Psychiatry.pdf', 0.0],\n",
       "       ['Memes-and-the-evolution-of-religion-We-need-memetics-too.pdf',\n",
       "        0.0],\n",
       "       ['manual_charge_2_en_US.pdf', 0.0],\n",
       "       ['log.txt', 0.0],\n",
       "       ['webcolors.txt', 0.0]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data:\n",
    "    for datum in row:\n",
    "        # Enter data in colums\n",
    "        # Notice the use of the function str to coerce any input to the\n",
    "        # string type. This is needed\n",
    "        # since pyFPDF expects a string, not a number.\n",
    "        pdf.cell(col_width, th, str(datum), border=1)\n",
    "    pdf.ln(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf.multi_cell(w=0, h=10, txt=d[['document', 'score']].to_string(index=False), align=\"L\")\n",
    "pdf.ln(10)\n",
    "\n",
    "# cater for small no of docs\n",
    "# cater for 0 scores\n",
    "topdocs = d.head(int(len(d)*0.1))\n",
    "\n",
    "# Print results of NER for people\n",
    "pdf.set_font('DejaVuSans-Bold', '', 12)\n",
    "print('People discovered:')\n",
    "pdf.multi_cell(w=0, h=10, txt='People discovered:', align=\"L\")\n",
    "pdf.set_font('DejaVu', '', 10)\n",
    "pdf.ln(5)\n",
    "for doc in topdocs['ner']:\n",
    "    for (a,b) in doc:\n",
    "        if b == 'PERSON':\n",
    "            print(a)\n",
    "            pdf.multi_cell(w=0, h=10, txt=a, align=\"L\")\n",
    "pdf.ln(10)\n",
    "\n",
    "# Print results of NER for organisations\n",
    "pdf.set_font('DejaVuSans-Bold', '', 12)\n",
    "print('Orgs discovered:')\n",
    "pdf.multi_cell(w=0, h=0, txt='Orgs discovered:', align=\"L\", border=1)\n",
    "pdf.set_font('DejaVu', '', 10)\n",
    "pdf.ln(5)\n",
    "for doc in topdocs['ner']:\n",
    "    for (a,b) in doc:\n",
    "        if b == 'ORG':\n",
    "            print(a)\n",
    "            pdf.multi_cell(w=0, h=10, txt=a, align=\"L\")\n",
    "\n",
    "# Output the case document with all the printed results to PDF\n",
    "pdf.output('C:\\\\tout\\\\simple_demo.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
